{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mercari Price Prediction Challenge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import joblib\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import optimizers, callbacks\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "user_input = [{\n",
    "    'name' : 'Levis Black Leggings, Womens. Size L.',\n",
    "    'item_condition_id' : 1,\n",
    "    'category_name' : 'Women/Athletic Apparel/Pants, Tights, Leggings',\n",
    "    'brand_name' : 'Levis',\n",
    "    'shipping' : 1,\n",
    "    'item_description' : 'Adorable gym wear from a well known brand. In great condition. Size L. Black and stretchable.'\n",
    "}, \n",
    "{\n",
    "    'name' : 'Coach bag',\n",
    "    'item_condition_id' : 1,\n",
    "    'category_name' : 'Vintage & Collectibles/Bags and Purses/Handbag',\n",
    "    'brand_name' : 'Coach',\n",
    "    'shipping' : 1,\n",
    "    'item_description' : 'Brand new coach bag. Bought for [rm] at a Coach outlet.'\n",
    "}]\n",
    "\n",
    "input_data = pd.DataFrame.from_dict(user_input)\n",
    "\n",
    "price = [10, 45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_c0 = joblib.load('/kaggle/input/picklefiles/vect-1.pickle')\n",
    "enc_c1 = joblib.load('/kaggle/input/picklefiles/vect-2.pickle')\n",
    "enc_c2 = joblib.load('/kaggle/input/picklefiles/vect-3.pickle')\n",
    "\n",
    "enc_n = joblib.load('/kaggle/input/picklefiles/vect-name.pickle')\n",
    "enc_t = joblib.load('/kaggle/input/picklefiles/vect-text.pickle')\n",
    "\n",
    "scaler = joblib.load('/kaggle/input/picklefiles/scaler.pickle') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords without no, not, etc\n",
    "STOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(data):\n",
    "    \n",
    "    data.fillna({'name':'missing', 'item_description':'missing'}, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(sentence):\n",
    "    \n",
    "    pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return pattern.sub(r'', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(sentence):\n",
    "    \n",
    "    regular_punct = list(string.punctuation)\n",
    "    \n",
    "    for punc in regular_punct:\n",
    "        if punc in sentence:\n",
    "            sentence = sentence.replace(punc, ' ')\n",
    "\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    \n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(data, cols):\n",
    "    \n",
    "    for col in cols:\n",
    "        \n",
    "        processed_data = []\n",
    "        \n",
    "        for sentence in data[col].values:\n",
    "            \n",
    "            sent = decontracted(sentence)\n",
    "            sent = sentence\n",
    "            sent = sent.replace('\\\\r', ' ')\n",
    "            sent = sent.replace('\\\\\"', ' ')\n",
    "            sent = sent.replace('\\\\n', ' ')\n",
    "            sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "            sent = remove_emoji(sent)\n",
    "            sent = remove_punctuation(sent)\n",
    "            sent = ' '.join(e for e in sent.split() if e not in STOPWORDS)\n",
    "            processed_data.append(sent.lower().strip())\n",
    "            \n",
    "        data[col] = processed_data\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category(data):\n",
    "    \n",
    "    for i in range(3):\n",
    "        \n",
    "        def get_part(x):\n",
    "            \n",
    "            if type(x) != str:\n",
    "                return np.nan\n",
    "        \n",
    "            parts = x.split('/')\n",
    "            \n",
    "            if i >= len(parts):\n",
    "                return np.nan\n",
    "            else:\n",
    "                return parts[i]\n",
    "\n",
    "        field_name = 'category_' + str(i)\n",
    "        \n",
    "        data[field_name] = data['category_name'].apply(get_part)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurerize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "    \n",
    "    luxury_brands = [\"MCM\", \"MCM Worldwide\", \"Louis Vuitton\", \"Burberry\", \"Burberry London\", \"Burberry Brit\", \"HERMES\", \"Tieks\",\n",
    "                     \"Rolex\", \"Apple\", \"Gucci\", \"Valentino\", \"Valentino Garavani\", \"RED Valentino\", \"Cartier\", \"Christian Louboutin\",\n",
    "                     \"Yves Saint Laurent\", \"Saint Laurent\", \"YSL Yves Saint Laurent\", \"Georgio Armani\", \"Armani Collezioni\", \"Emporio Armani\"]\n",
    "    \n",
    "    data['is_luxurious'] = (data['brand_name'].isin(luxury_brands)).astype(np.int8)\n",
    "\n",
    "    expensive_brands = [\"Michael Kors\", \"Louis Vuitton\", \"Lululemon\", \"LuLaRoe\", \"Kendra Scott\", \"Tory Burch\", \"Apple\", \"Kate Spade\",\n",
    "                  \"UGG Australia\", \"Coach\", \"Gucci\", \"Rae Dunn\", \"Tiffany & Co.\", \"Rock Revival\", \"Adidas\", \"Beats\", \"Burberry\",\n",
    "                  \"Christian Louboutin\", \"David Yurman\", \"Ray-Ban\", \"Chanel\"]\n",
    "\n",
    "    data['is_expensive'] = (data['brand_name'].isin(expensive_brands)).astype(np.int8)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "\n",
    "    #handle missing values\n",
    "    data = handle_missing_values(data)\n",
    "    \n",
    "    data = process_category(data)\n",
    "    \n",
    "    data = process_text(data, ['name', 'item_description', 'category_name'])\n",
    "    \n",
    "    data = get_features(data)\n",
    "    \n",
    "    data.fillna({'brand_name': ' ', 'category_0': 'other', 'category_1': 'other', 'category_2': 'other'}, inplace = True)\n",
    "    \n",
    "    #concat columns\n",
    "    data['name'] = data['name'] + ' ' + data['brand_name'] + ' ' + data['category_name'] \n",
    "    data['text'] = data['name'] + ' ' + data['item_description']\n",
    "    \n",
    "    #drop columns which are not required\n",
    "    data = data.drop(columns = ['brand_name', 'item_description', 'category_name'], axis = 1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode features\n",
    "def get_encodings(data):\n",
    "    \n",
    "    category_0 = enc_c0.transform(data['category_0'].values)\n",
    "\n",
    "    category_1 = enc_c1.transform(data['category_1'].values)\n",
    "\n",
    "    category_2 = enc_c2.transform(data['category_2'].values)\n",
    "    \n",
    "    nums = csr_matrix(pd.get_dummies(data[['shipping', 'item_condition_id', 'is_expensive', 'is_luxurious']], sparse=True).values)\n",
    "\n",
    "    name = enc_n.transform(data['name'].values)\n",
    "    \n",
    "    text = enc_t.transform(data['text'].values)\n",
    "\n",
    "    data = hstack((category_0, category_1, category_2, nums, name, text)).tocsr().astype('float32')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_regr():\n",
    "\n",
    "    model_in = Input(shape = (363097,), dtype = 'float32', sparse = True)\n",
    "    out = Dense(256, activation = 'relu')(model_in)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Dense(128, activation = 'relu')(out)\n",
    "    out = Dense(64, activation = 'relu')(out)\n",
    "    out = Dense(32, activation = 'relu')(out)\n",
    "    out = Dense(16, activation = 'relu')(out)\n",
    "    model_out = Dense(1)(out)\n",
    "    \n",
    "    model = Model(model_in, model_out)\n",
    "    \n",
    "    model.compile(loss = 'mean_squared_error', optimizer = keras.optimizers.Adam(learning_rate=0.001))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_regr():\n",
    "    \n",
    "    model_in = Input(shape = (363097,), dtype = 'float32', sparse = True)\n",
    "    out = Dense(1024, activation='relu')(model_in)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Dense(512, activation='relu')(out)\n",
    "    out = Dropout(0.2)(out)\n",
    "    out = Dense(256, activation='relu')(out)\n",
    "    out = Dense(128, activation='relu')(out)\n",
    "    out = Dense(64, activation='relu')(out)\n",
    "    out = Dense(32, activation='relu')(out)\n",
    "    out = Dense(16, activation='relu')(out)\n",
    "    model_out = Dense(1)(out)\n",
    "    \n",
    "    model = Model(model_in, model_out)\n",
    "    \n",
    "    model.compile(loss = 'mean_squared_error', optimizer = keras.optimizers.Adam(learning_rate=0.001))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "def final_fun_1(input_data):\n",
    "    \n",
    "    print('Processing Data...')\n",
    "    processed_data = preprocess(input_data)\n",
    "\n",
    "    print('Generating Encodings...')\n",
    "    encoded_data = get_encodings(processed_data)\n",
    "\n",
    "    print('Making Predictions...')\n",
    "    model = f_regr()\n",
    "    model.load_weights('/kaggle/input/picklefiles/model-1.h5')\n",
    "    pred = model.predict(encoded_data)\n",
    "    pred_nn_1 = np.expm1(scaler.inverse_transform(pred.reshape(-1, 1))[:,0])\n",
    "\n",
    "    model = s_regr()\n",
    "    model.load_weights('/kaggle/input/picklefiles/model-2.h5')\n",
    "    pred = model.predict(encoded_data)\n",
    "    pred_nn_2 = np.expm1(scaler.inverse_transform(pred.reshape(-1, 1))[:,0])\n",
    "    \n",
    "    w = 0.35\n",
    "    prediction = (w * pred_nn_1) + ((1 - w) * pred_nn_2)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1\n",
    "\n",
    "#### Returns the predicted price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    prediction = final_fun_1(input_data)\n",
    "    \n",
    "    for pred in prediction:\n",
    "        print('Predicted price is ${:.2f}'.format(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data...\n",
      "Generating Encodings...\n",
      "Making Predictions...\n",
      "Predicted price is $9.79\n",
      "Predicted price is $38.10\n",
      "CPU times: user 8.1 s, sys: 12 s, total: 20.1 s\n",
      "Wall time: 8.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2\n",
    "\n",
    "#### Returns the RMSLE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_2(X, y):\n",
    "    \n",
    "    return get_rmsle(y, final_fun_1(X))\n",
    "    \n",
    "    \n",
    "def get_rmsle(y_true, y_pred):\n",
    "    \n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    rmsle = final_fun_2(input_data, price)\n",
    "    \n",
    "    print('Computed RMSLE is {:.3f}'.format(rmsle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data...\n",
      "Generating Encodings...\n",
      "Making Predictions...\n",
      "Computed RMSLE is 0.636\n",
      "CPU times: user 6.47 s, sys: 7.43 s, total: 13.9 s\n",
      "Wall time: 6.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
